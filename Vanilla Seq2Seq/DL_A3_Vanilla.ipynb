{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kbXtC0XQDHtI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Imports\n",
        "'''\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the Dakshina Dataset\n",
        "\n",
        "'''\n",
        "Downloading the data\n",
        "'''\n",
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "\n",
        "'''\n",
        "Capturing the data and saving as the Tar file\n",
        "'''\n",
        "!tar -xvf  'daksh.tar'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9HUvsW3DeiH",
        "outputId": "631a688b-558d-4c07-986b-a011b606ae7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0  22.2M      0  0:01:26  0:01:26 --:--:-- 22.9M\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeTensor(texts, tokenizer=None):\n",
        "    if tokenizer is None:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True, filters='')\n",
        "        tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    encoded = tokenizer.texts_to_sequences(texts)\n",
        "    padded_result = tf.keras.preprocessing.sequence.pad_sequences(encoded, padding='post')\n",
        "\n",
        "    return padded_result, tokenizer\n",
        "\n",
        "'''\n",
        "Function to read the data\n",
        "Input - Data path to read the data\n",
        "Output - input text, target text, input and target tokenizier, input and target tensor\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "def data(path, input_tokenizer=None, output_tokenizer=None, input_length=None, output_length=None):\n",
        "    input_texts = []   # stores source sentences\n",
        "    target_texts = []  # stores target sentences\n",
        "\n",
        "    # Load and shuffle the dataset if tokenizers are not provided\n",
        "    dataset = pd.read_csv(path, sep=\"\\t\", names=[\"col1\", \"col2\", \"col3\"]).astype(str)\n",
        "    if input_tokenizer is None:\n",
        "        dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # Preprocess each row\n",
        "    for _, row in dataset.iterrows():\n",
        "        source = row[\"col3\"]\n",
        "        target = row[\"col2\"]\n",
        "        if source == '</s>' or target == '</s>':\n",
        "            continue\n",
        "        # Add start and end tokens to target\n",
        "        target = \"\\t\" + target + \"\\n\"\n",
        "        input_texts.append(source)\n",
        "        target_texts.append(target)\n",
        "\n",
        "    # Tokenize the inputs and outputs\n",
        "    input_tensor, input_tokenizer = tokenizeTensor(input_texts, input_tokenizer)\n",
        "    target_tensor, output_tokenizer = tokenizeTensor(target_texts, output_tokenizer)\n",
        "\n",
        "    # Optional: pad tensors to specified lengths\n",
        "    if input_length is not None and output_length is not None:\n",
        "        input_padding = input_length - input_tensor.shape[1]\n",
        "        output_padding = output_length - target_tensor.shape[1]\n",
        "\n",
        "        if input_padding > 0:\n",
        "            input_tensor = tf.concat(\n",
        "                [input_tensor, tf.zeros((input_tensor.shape[0], input_padding), dtype=input_tensor.dtype)],\n",
        "                axis=1\n",
        "            )\n",
        "        if output_padding > 0:\n",
        "            target_tensor = tf.concat(\n",
        "                [target_tensor, tf.zeros((target_tensor.shape[0], output_padding), dtype=target_tensor.dtype)],\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "    # Return texts, tensors, and tokenizers\n",
        "    return input_texts, input_tensor, input_tokenizer, target_texts, target_tensor, output_tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "MVQnAMFqDqpY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing and reading the training data\n",
        "%%capture\n",
        "input_texts,input_tensor,input_tokenizer,target_texts,target_tensor,target_tokenizer=data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "A7dCPkhzEGVX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing and reading the validation data\n",
        "%%capture\n",
        "val_input_texts,val_input_tensor,val_input_tokenizer,val_target_texts,val_target_tensor,val_target_tokenizer=data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\",input_tokenizer,target_tokenizer,input_tensor.shape[1],target_tensor.shape[1])\n"
      ],
      "metadata": {
        "id": "F-jWddnSFHvI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing and reading the testing data\n",
        "%%capture\n",
        "test_input_texts,test_input_tensor,test_input_tokenizer,test_target_texts,test_target_tensor,test_target_tokenizer=data(\"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\",input_tokenizer,target_tokenizer,input_tensor.shape[1],target_tensor.shape[1])\n"
      ],
      "metadata": {
        "id": "uucemywpFHkL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Token and Sequence Configuration ---\n",
        "encoder_vocab_size = len(input_tokenizer.word_index) + 1  # encoder vocabulary size\n",
        "decoder_vocab_size = len(target_tokenizer.word_index) + 1  # decoder vocabulary size\n",
        "max_encoder_length = input_tensor.shape[1]                # maximum encoder sequence length\n",
        "max_decoder_length = target_tensor.shape[1]                # maximum decoder sequence length\n",
        "\n",
        "# Reverse lookup: index -> character\n",
        "encoder_index_to_char = {idx: char for char, idx in input_tokenizer.word_index.items()}\n",
        "decoder_index_to_char = {idx: char for char, idx in target_tokenizer.word_index.items()}\n",
        "\n",
        "# --- Model Builder ---\n",
        "def build_model(rnn_type, embedding_dim, encoder_layers, decoder_layers, dropout):\n",
        "    \"\"\"\n",
        "    Constructs a sequence-to-sequence model with configurable RNN cell type.\n",
        "\n",
        "    Args:\n",
        "        rnn_type (str): 'LSTM', 'GRU', or 'RNN'.\n",
        "        embedding_dim (int): dimension of embedding vectors.\n",
        "        encoder_layers (int): number of stacked encoder RNN layers.\n",
        "        decoder_layers (int): number of stacked decoder RNN layers.\n",
        "        dropout (float): dropout rate for all RNN layers.\n",
        "\n",
        "    Returns:\n",
        "        keras.Model: compiled seq2seq model.\n",
        "    \"\"\"\n",
        "    # Map type string to actual Keras layer class\n",
        "    cell_map = {\n",
        "        'LSTM': keras.layers.LSTM,\n",
        "        'GRU': keras.layers.GRU,\n",
        "        'RNN': keras.layers.SimpleRNN\n",
        "    }\n",
        "    rnn_cell = cell_map.get(rnn_type)\n",
        "    assert rnn_cell, f\"Unsupported rnn_type: {rnn_type}\"\n",
        "\n",
        "    # --- Encoder ---\n",
        "    enc_inputs = keras.Input(shape=(max_encoder_length,), name='encoder_inputs')\n",
        "    enc_emb = keras.layers.Embedding(encoder_vocab_size, embedding_dim, name='encoder_embedding')(enc_inputs)\n",
        "\n",
        "    # Stack RNN layers for encoder\n",
        "    enc_output = enc_emb\n",
        "    enc_states = []\n",
        "    for i in range(encoder_layers):\n",
        "        is_final = (i == encoder_layers - 1)\n",
        "        if is_final:\n",
        "            # final encoder returns states\n",
        "            if rnn_type == 'LSTM':\n",
        "                enc_out, state_h, state_c = rnn_cell(latent_dim,\n",
        "                                                     return_sequences=False,\n",
        "                                                     return_state=True,\n",
        "                                                     dropout=dropout,\n",
        "                                                     name=f'encoder_{i}')(enc_output)\n",
        "                enc_states = [state_h, state_c]\n",
        "            else:\n",
        "                enc_out, state = rnn_cell(latent_dim,\n",
        "                                          return_sequences=False,\n",
        "                                          return_state=True,\n",
        "                                          dropout=dropout,\n",
        "                                          name=f'encoder_{i}')(enc_output)\n",
        "                enc_states = [state]\n",
        "        else:\n",
        "            # intermediate encoder returns full sequence\n",
        "            enc_out = rnn_cell(latent_dim,\n",
        "                               return_sequences=True,\n",
        "                               dropout=dropout,\n",
        "                               name=f'encoder_{i}')(enc_output)\n",
        "        enc_output = enc_out\n",
        "\n",
        "    # --- Decoder ---\n",
        "    dec_inputs = keras.Input(shape=(max_decoder_length,), name='decoder_inputs')\n",
        "    dec_emb = keras.layers.Embedding(decoder_vocab_size, embedding_dim, name='decoder_embedding')(dec_inputs)\n",
        "\n",
        "    dec_output = dec_emb\n",
        "    last_dec_output = None\n",
        "    for j in range(decoder_layers):\n",
        "        is_final_dec = (j == decoder_layers - 1)\n",
        "        return_seq = True  # always return sequence for stacking\n",
        "        if is_final_dec:\n",
        "            # final decoder, we only need output sequences (we don't use states further)\n",
        "            return_state_flag = False\n",
        "        else:\n",
        "            return_state_flag = False\n",
        "\n",
        "        # invoke decoder cell with initial state from encoder\n",
        "        if rnn_type == 'LSTM':\n",
        "            dec_cell = keras.layers.LSTM(latent_dim,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         dropout=dropout,\n",
        "                                         name=f'decoder_{j}')\n",
        "            if j == 0:\n",
        "                out_seq, _, _ = dec_cell(dec_output, initial_state=enc_states)\n",
        "            else:\n",
        "                out_seq, _, _ = dec_cell(last_dec_output, initial_state=enc_states)\n",
        "        else:\n",
        "            dec_cell = cell_map[rnn_type](latent_dim,\n",
        "                                          return_sequences=True,\n",
        "                                          return_state=True,\n",
        "                                          dropout=dropout,\n",
        "                                          name=f'decoder_{j}')\n",
        "            if j == 0:\n",
        "                out_seq, _ = dec_cell(dec_output, initial_state=enc_states)\n",
        "            else:\n",
        "                out_seq, _ = dec_cell(last_dec_output, initial_state=enc_states)\n",
        "        last_dec_output = out_seq\n",
        "\n",
        "    # Final dense projection\n",
        "    dec_dense = keras.layers.Dense(decoder_vocab_size, activation='softmax', name='decoder_output_dense')\n",
        "    dec_outputs = dec_dense(last_dec_output)\n",
        "\n",
        "    # Define and return the model\n",
        "    model = keras.Model([enc_inputs, dec_inputs], dec_outputs, name='seq2seq_model')\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Function - inferencing\n",
        "Inputs -\n",
        "  model\n",
        "  encoder_layers\n",
        "  decoder_layers\n",
        "Output - encoder model and the deocder model separately\n",
        "'''\n",
        "\n",
        "\n",
        "def inferencing(model, encoder_layers, decoder_layers):\n",
        "    \"\"\"\n",
        "    Split a trained seq2seq model into separate encoder and decoder inference models.\n",
        "\n",
        "    Args:\n",
        "        model (keras.Model): trained seq2seq model.\n",
        "        encoder_layers (int): number of encoder RNN layers used during training.\n",
        "        decoder_layers (int): number of decoder RNN layers used during training.\n",
        "\n",
        "    Returns:\n",
        "        (encoder_model, decoder_model): two keras Models for inference.\n",
        "    \"\"\"\n",
        "    # --- Encoder Inference ---\n",
        "    encoder_input = model.input[0]\n",
        "\n",
        "    # Locate final encoder layer by offset from inputs\n",
        "    # Encoder cells follow: Input -> embedding -> encoder layers...\n",
        "    encoder_cell = model.layers[2 + encoder_layers]\n",
        "    if isinstance(encoder_cell, keras.layers.LSTM):\n",
        "        _, state_h, state_c = encoder_cell.output\n",
        "        encoder_states = [state_h, state_c]\n",
        "    else:\n",
        "        _, state = encoder_cell.output\n",
        "        encoder_states = [state]\n",
        "\n",
        "    encoder_model = keras.Model(inputs=encoder_input, outputs=encoder_states,\n",
        "                                name='encoder_inference')\n",
        "\n",
        "    # --- Decoder Inference ---\n",
        "    # Single-step decoder input token\n",
        "    single_dec_input = keras.Input(shape=(1,), name='dec_input_token')\n",
        "    # Reuse embedding from trained model\n",
        "    dec_embed_layer = model.get_layer('decoder_embedding')\n",
        "    dec_input_embedded = dec_embed_layer(single_dec_input)\n",
        "\n",
        "    # Prepare placeholders for decoder initial states\n",
        "    dec_state_inputs = []\n",
        "    dec_states_outputs = []\n",
        "    prev_output = None\n",
        "\n",
        "    for idx in range(decoder_layers):\n",
        "        # Identify the trained decoder cell by name or position\n",
        "        cell_layer = model.get_layer(f'decoder_{idx}')\n",
        "\n",
        "        if isinstance(cell_layer, keras.layers.LSTM):\n",
        "            # Two state vectors for LSTM\n",
        "            h_input = keras.Input(shape=(latent_dim,), name=f'dec_h_in_{idx}')\n",
        "            c_input = keras.Input(shape=(latent_dim,), name=f'dec_c_in_{idx}')\n",
        "            init_states = [h_input, c_input]\n",
        "\n",
        "            if idx == 0:\n",
        "                out_seq, h_out, c_out = cell_layer(dec_input_embedded,\n",
        "                                                   initial_state=encoder_states)\n",
        "            else:\n",
        "                out_seq, h_out, c_out = cell_layer(prev_output,\n",
        "                                                   initial_state=init_states)\n",
        "\n",
        "            dec_state_inputs += [h_input, c_input]\n",
        "            dec_states_outputs += [h_out, c_out]\n",
        "\n",
        "        else:\n",
        "            # Single state for GRU or SimpleRNN\n",
        "            s_input = keras.Input(shape=(latent_dim,), name=f'dec_s_in_{idx}')\n",
        "            init_states = [s_input]\n",
        "\n",
        "            if idx == 0:\n",
        "                out_seq, s_out = cell_layer(dec_input_embedded,\n",
        "                                            initial_state=encoder_states)\n",
        "            else:\n",
        "                out_seq, s_out = cell_layer(prev_output,\n",
        "                                            initial_state=init_states)\n",
        "\n",
        "            dec_state_inputs.append(s_input)\n",
        "            dec_states_outputs.append(s_out)\n",
        "\n",
        "        prev_output = out_seq\n",
        "\n",
        "    # Final dense layer for token probabilities\n",
        "    final_dense = model.get_layer('final')\n",
        "    dec_token_probs = final_dense(prev_output)\n",
        "\n",
        "    # Construct decoder inference model\n",
        "    decoder_model = keras.Model(\n",
        "        inputs=[single_dec_input] + dec_state_inputs,\n",
        "        outputs=[dec_token_probs] + dec_states_outputs,\n",
        "        name='decoder_inference'\n",
        "    )\n",
        "\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def do_predictions(input_seq, encoder_model, decoder_model,\n",
        "                   batch_size, encoder_layers, decoder_layers,\n",
        "                   target_tokenizer, index_to_char_target,\n",
        "                   max_decoder_seq_length, rnn_type='LSTM'):\n",
        "    \"\"\"\n",
        "    Decode a batch of sequences to generate target predictions.\n",
        "    \"\"\"\n",
        "    # Encode input and prepare initial decoder states\n",
        "    states = encoder_model.predict(input_seq)\n",
        "    # Ensure states is a list for RNN/GRU\n",
        "    if rnn_type in ('GRU', 'RNN'):\n",
        "        states = [states]\n",
        "    # Repeat state list for each decoder layer\n",
        "    decoder_states = states * decoder_layers\n",
        "\n",
        "    # Initialize target indices with start token '\\t'\n",
        "    start_idx = target_tokenizer.word_index['\\t']\n",
        "    prev_indices = np.full((batch_size, 1), start_idx, dtype=int)\n",
        "\n",
        "    # Prepare containers for outputs\n",
        "    predictions = [''] * batch_size\n",
        "    finished = np.zeros(batch_size, dtype=bool)\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "        # Predict next token and new states\n",
        "        outputs = decoder_model.predict([prev_indices] + decoder_states)\n",
        "        probs = outputs[0]\n",
        "        decoder_states = outputs[1:]\n",
        "\n",
        "        # Choose highest-probability token for each sequence\n",
        "        next_indices = np.argmax(probs[:, -1, :], axis=-1)\n",
        "        prev_indices[:, 0] = next_indices\n",
        "\n",
        "        for i, token_idx in enumerate(next_indices):\n",
        "            if finished[i]:\n",
        "                continue\n",
        "            # Map token index to character\n",
        "            if token_idx == 0:\n",
        "                char = '\\n'\n",
        "            else:\n",
        "                char = index_to_char_target[token_idx]\n",
        "\n",
        "            # Check for end token\n",
        "            if char == '\\n':\n",
        "                finished[i] = True\n",
        "            else:\n",
        "                predictions[i] += char\n",
        "\n",
        "        # Stop early if all sequences have finished\n",
        "        if finished.all():\n",
        "            break\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def test_accuracy(encoder_model, decoder_model,\n",
        "                  test_input_tensor, test_input_texts, test_target_texts,\n",
        "                  target_tokenizer, index_to_char_target,\n",
        "                  encoder_layers, decoder_layers,\n",
        "                  max_decoder_seq_length, rnn_type='LSTM',\n",
        "                  success_path=\"success_predictions.txt\",\n",
        "                  failure_path=\"failure_predictions.txt\"):\n",
        "    \"\"\"\n",
        "    Compute word-level accuracy on the test set, logging successes and failures.\n",
        "    \"\"\"\n",
        "    batch_size = test_input_tensor.shape[0]\n",
        "    preds = do_predictions(\n",
        "        input_seq=test_input_tensor,\n",
        "        encoder_model=encoder_model,\n",
        "        decoder_model=decoder_model,\n",
        "        batch_size=batch_size,\n",
        "        encoder_layers=encoder_layers,\n",
        "        decoder_layers=decoder_layers,\n",
        "        target_tokenizer=target_tokenizer,\n",
        "        index_to_char_target=index_to_char_target,\n",
        "        max_decoder_seq_length=max_decoder_seq_length,\n",
        "        rnn_type=rnn_type\n",
        "    )\n",
        "\n",
        "    success_count = 0\n",
        "    # Open files once\n",
        "    with open(success_path, 'a') as succ_f, open(failure_path, 'a') as fail_f:\n",
        "        for inp_text, true_text, pred in zip(\n",
        "                test_input_texts, test_target_texts, preds):\n",
        "            true = true_text[1:-1]\n",
        "            if pred == true:\n",
        "                success_count += 1\n",
        "                succ_f.write(f\"{inp_text} {true} {pred}\\n\")\n",
        "            else:\n",
        "                fail_f.write(f\"{inp_text} {true} {pred}\\n\")\n",
        "\n",
        "    return success_count / batch_size\n",
        "\n",
        "\n",
        "def batch_validate(encoder_model, decoder_model,\n",
        "                   val_input_tensor, val_target_texts,\n",
        "                   target_tokenizer, index_to_char_target,\n",
        "                   encoder_layers, decoder_layers,\n",
        "                   max_decoder_seq_length, rnn_type='LSTM'):\n",
        "    \"\"\"\n",
        "    Validate on the entire validation batch and return accuracy.\n",
        "    \"\"\"\n",
        "    batch_size = val_input_tensor.shape[0]\n",
        "    preds = do_predictions(\n",
        "        input_seq=val_input_tensor,\n",
        "        encoder_model=encoder_model,\n",
        "        decoder_model=decoder_model,\n",
        "        batch_size=batch_size,\n",
        "        encoder_layers=encoder_layers,\n",
        "        decoder_layers=decoder_layers,\n",
        "        target_tokenizer=target_tokenizer,\n",
        "        index_to_char_target=index_to_char_target,\n",
        "        max_decoder_seq_length=max_decoder_seq_length,\n",
        "        rnn_type=rnn_type\n",
        "    )\n",
        "\n",
        "    correct = sum(\n",
        "        pred == target[1:-1]\n",
        "        for pred, target in zip(preds, val_target_texts)\n",
        "    )\n",
        "\n",
        "    return correct / batch_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Training for Wandb Hyper parameter sweeping\n",
        "\n",
        "#defining globals\n",
        "rnn_type=None\n",
        "embedding_dim=None\n",
        "model= None\n",
        "latent_dim = None\n",
        "enc_layers=None\n",
        "dec_layers=None\n",
        "'''\n",
        "Function- train()\n",
        "Performs the entire training using Wandb sweeps\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train():\n",
        "    global rnn_type\n",
        "    global embedding_dim\n",
        "    global model\n",
        "    global latent_dim\n",
        "    global enc_layer\n",
        "    global dec_layer\n",
        "\n",
        "    # Initialize Weights and Biases\n",
        "    wandb.init()\n",
        "\n",
        "    # Assign configuration values to global variables\n",
        "    rnn_type = wandb.config.rnn_type\n",
        "    embedding_dim = wandb.config.embedding_dim\n",
        "    latent_dim = wandb.config.latent_dim\n",
        "    enc_layer = wandb.config.enc_layer\n",
        "    dec_layer = wandb.config.dec_layer\n",
        "    dropout = wandb.config.dropout\n",
        "    epochs = wandb.config.epochs\n",
        "    bs = wandb.config.bs\n",
        "\n",
        "    # Set a descriptive name for the current run\n",
        "    wandb.run.name = f\"epochs_{epochs}_bs_{bs}_rnn_type_{rnn_type}_em_{embedding_dim}_latd_{latent_dim}_encs_{enc_layer}_decs_{dec_layer}_dr_{dropout}\"\n",
        "\n",
        "    # Construct and compile the model\n",
        "    model = build_model(\n",
        "        rnn_type=rnn_type,\n",
        "        embedding_dim=embedding_dim,\n",
        "        encoder_layers=enc_layer,\n",
        "        decoder_layers=dec_layer,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(reduction=\"none\"),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    # Train for specified number of epochs\n",
        "    for epoch_index in range(epochs):\n",
        "        history = model.fit(\n",
        "            [input_tensor, target_tensor],\n",
        "            tf.concat([target_tensor[:, 1:], tf.zeros((target_tensor.shape[0], 1))], axis=1),\n",
        "            batch_size=bs,\n",
        "            epochs=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # Save model after each epoch\n",
        "        model.save(\"vanilla.keras\")\n",
        "\n",
        "        # Load the saved model and setup inference models\n",
        "        loaded_model = keras.models.load_model(\"/content/vanilla.keras\")\n",
        "        encoder_model, decoder_model = inferencing(\n",
        "            loaded_model,\n",
        "            encoder_layers=enc_layer,\n",
        "            decoder_layers=dec_layer\n",
        "        )\n",
        "\n",
        "        # Log training loss to wandb\n",
        "        wandb.log({\"train_loss\": history.history['loss'][0]})\n",
        "\n",
        "    # Compute validation accuracy and log to wandb\n",
        "    validation_accuracy = batch_validate(encoder_model, decoder_model, enc_layer, dec_layer)\n",
        "    wandb.log({\"val_acc\": validation_accuracy})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Manual Training\n",
        "\n",
        "#defining globals\n",
        "rnn_type=None\n",
        "embedding_dim=None\n",
        "model= None\n",
        "latent_dim = None\n",
        "enc_layers=None\n",
        "dec_layers=None\n",
        "'''\n",
        "Function - Manual Train\n",
        "perform the training manually for the best configuration\n",
        "'''\n",
        "\n",
        "def manual_train(config):\n",
        "    global rnn_type\n",
        "    global embedding_dim\n",
        "    global model\n",
        "    global latent_dim\n",
        "    global enc_layer\n",
        "    global dec_layer\n",
        "\n",
        "    # Assign hyperparameters from the provided config\n",
        "    rnn_type = config.rnn_type\n",
        "    embedding_dim = config.embedding_dim\n",
        "    latent_dim = config.latent_dim\n",
        "    enc_layer = config.enc_layer\n",
        "    dec_layer = config.dec_layer\n",
        "    dropout = config.dropout\n",
        "    epochs = config.epochs\n",
        "    bs = config.bs\n",
        "\n",
        "    # Construct the sequence-to-sequence model\n",
        "    model = build_model(\n",
        "        rnn_type=rnn_type,\n",
        "        embedding_dim=embedding_dim,\n",
        "        encoder_layers=enc_layer,\n",
        "        decoder_layers=dec_layer,\n",
        "        dropout=dropout\n",
        "    )\n",
        "\n",
        "    # Compile model with specified loss and optimizer\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(reduction=\"none\"),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    # Save a visualization of the model architecture\n",
        "    tf.keras.utils.plot_model(\n",
        "        model,\n",
        "        to_file='model.png',\n",
        "        show_shapes=True,\n",
        "        show_dtype=True,\n",
        "        show_layer_names=True,\n",
        "        dpi=96\n",
        "    )\n",
        "\n",
        "    ###################################################### Training Loop ######################################################\n",
        "    for epoch_num in range(epochs):\n",
        "        history = model.fit(\n",
        "            [input_tensor, target_tensor],\n",
        "            tf.concat(\n",
        "                [target_tensor[:, 1:], tf.zeros((target_tensor.shape[0], 1))],\n",
        "                axis=1\n",
        "            ),\n",
        "            batch_size=bs,\n",
        "            epochs=1,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # Save model to disk after each epoch\n",
        "        model.save(\"vanilla.keras\")\n",
        "\n",
        "        # Load model and extract encoder/decoder components for evaluation\n",
        "        trained_model = keras.models.load_model(\"/content/vanilla.keras\")\n",
        "        encoder_model, decoder_model = inferencing(\n",
        "            trained_model,\n",
        "            encoder_layers=enc_layer,\n",
        "            decoder_layers=dec_layer\n",
        "        )\n",
        "\n",
        "        # Compute and display validation accuracy\n",
        "        val_acc = batch_validate(encoder_model, decoder_model, enc_layer, dec_layer)\n",
        "        print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "    # Final evaluation on test data\n",
        "    test_acc = test_accuracy(encoder_model, decoder_model, enc_layer, dec_layer)\n",
        "    print(\"Test Accuracy:\", test_acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Install wandb (only needs to be done once)\n",
        "!pip install -q wandb\n",
        "\n",
        "# Import wandb after installation\n",
        "import wandb\n",
        "\n",
        "# Enable wandb logging\n",
        "wb = True\n",
        "\n",
        "if wb:\n",
        "    wandb.login(key=\"7f46816d45e3df192c3053bab59032e9d710fef4\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# generating the wandb sweep configuration\n",
        "if wb:\n",
        "  sweep_config = {\n",
        "    \"name\": \"Bayesian Sweep without attention\",\n",
        "    \"method\": \"bayes\", #method used was bayesian\n",
        "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"}, #mximizing the validation accuracy\n",
        "    \"parameters\": {\n",
        "\n",
        "        \"rnn_type\": {\"values\": [\"LSTM\"]}, #GRU, RNN\n",
        "\n",
        "        \"embedding_dim\": {\"values\": [128,256,512]},\n",
        "\n",
        "        \"latent_dim\": {\"values\": [128,256,512,1024]},\n",
        "\n",
        "        \"enc_layer\": {\"values\": [1, 2, 3]},\n",
        "\n",
        "        \"dec_layer\": {\"values\": [1, 2, 3]},\n",
        "\n",
        "        \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n",
        "\n",
        "        \"epochs\": {\"values\": [20]},\n",
        "\n",
        "        \"bs\": {\"values\": [64]},\n",
        "\n",
        "\n",
        "    },\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #creating the wandb sweep\n",
        "  sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assignment3_vanilla\", entity=\"cs24m034-indian-institute-of-technology-madras\")\n",
        "  #calling the wandb sweep to start the hyper parameter tuning.\n",
        "  wandb.agent(sweep_id, train, count = 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "5QHxcAqWFQr6",
        "outputId": "a95dc403-9dcd-47bd-e922-5105d3908f41"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: r2okq6p7\n",
            "Sweep URL: https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla/sweeps/r2okq6p7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qskkzzmm with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbs: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layer: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layer: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dim: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: LSTM\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250521_183043-qskkzzmm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla/runs/qskkzzmm' target=\"_blank\">grateful-sweep-1</a></strong> to <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla/sweeps/r2okq6p7' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla/sweeps/r2okq6p7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla/sweeps/r2okq6p7' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla/sweeps/r2okq6p7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla/runs/qskkzzmm' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_vanilla/runs/qskkzzmm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m157/691\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 31ms/step - accuracy: 0.6443 - loss: 1.3939"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5LwrCNcoFWWR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QcT7F-s60AXx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}