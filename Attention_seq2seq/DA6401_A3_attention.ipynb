{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "KVcLJT6jXqdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "aAx2Mf3DFpQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLe05yXv7jQO",
        "outputId": "221191ff-4dd1-4b3b-898a-df2bfc7d319e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uniseg in /usr/local/lib/python3.11/dist-packages (0.10.0)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Imports\n",
        "'''\n",
        "!pip install uniseg\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import shutil\n",
        "#HTMl library to generate the connectivity html file\n",
        "from IPython.display import HTML as html_print\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfr4kTIE7wop",
        "outputId": "d16b2f4c-8897-496b-e4d2-e694bf99db6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0   107M      0  0:00:17  0:00:17 --:--:-- 50.8M\n",
            "dakshina_dataset_v1.0/bn/\n",
            "dakshina_dataset_v1.0/bn/lexicons/\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/native_script_wikipedia/bn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/bn/romanized/\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/bn/romanized/bn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/\n",
            "dakshina_dataset_v1.0/gu/lexicons/\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/gu/lexicons/gu.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/native_script_wikipedia/gu.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/gu/romanized/\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/gu/romanized/gu.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/\n",
            "dakshina_dataset_v1.0/hi/lexicons/\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/hi/native_script_wikipedia/hi.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/hi/romanized/\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/hi/romanized/hi.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/\n",
            "dakshina_dataset_v1.0/kn/lexicons/\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/kn/lexicons/kn.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/kn/native_script_wikipedia/kn.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/kn/romanized/\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/kn/romanized/kn.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/\n",
            "dakshina_dataset_v1.0/ml/lexicons/\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.rom.txt\n",
            "dakshina_dataset_v1.0/ml/lexicons/tmp.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ml/lexicons/ml.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ml/native_script_wikipedia/ml.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ml/romanized/\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ml/romanized/ml.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/\n",
            "dakshina_dataset_v1.0/mr/lexicons/\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/mr/lexicons/mr.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/native_script_wikipedia/mr.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/mr/romanized/\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/mr/romanized/mr.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/\n",
            "dakshina_dataset_v1.0/pa/lexicons/\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/pa/lexicons/pa.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/pa/native_script_wikipedia/pa.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/pa/romanized/\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/pa/romanized/pa.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/\n",
            "dakshina_dataset_v1.0/sd/lexicons/\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/sd/lexicons/sd.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/sd/native_script_wikipedia/sd.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/sd/romanized/\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/sd/romanized/sd.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/\n",
            "dakshina_dataset_v1.0/si/lexicons/\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/si/lexicons/si.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/native_script_wikipedia/si.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/si/romanized/\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/si/romanized/si.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/\n",
            "dakshina_dataset_v1.0/ta/lexicons/\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ta/native_script_wikipedia/ta.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ta/romanized/\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ta/romanized/ta.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/\n",
            "dakshina_dataset_v1.0/te/lexicons/\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/native_script_wikipedia/te.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/te/romanized/\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/te/romanized/te.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/ur/\n",
            "dakshina_dataset_v1.0/ur/lexicons/\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\n",
            "dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.omit_pages.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.urls.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-full.nonblock.sections.list.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.valid.text.shuf.txt.gz\n",
            "dakshina_dataset_v1.0/ur/native_script_wikipedia/ur.wiki-filt.train.info.sorted.tsv.gz\n",
            "dakshina_dataset_v1.0/ur/romanized/\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.roman.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.cased_nopunct.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.dev.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.native.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.validation.edits.txt\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.aligned.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.split.tsv\n",
            "dakshina_dataset_v1.0/ur/romanized/ur.romanized.rejoined.test.roman.txt\n",
            "dakshina_dataset_v1.0/README.md\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Downloading the dataset\n",
        "'''\n",
        "# Download the dataset\n",
        "!curl https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar --output daksh.tar\n",
        "# Extract the downloaded tar file\n",
        "!tar -xvf  'daksh.tar'\n",
        "# Set the file paths to train, validation and test dataset\n",
        "#train_path\n",
        "train_file_path=os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.train.tsv\")\n",
        "#validation_path\n",
        "vaildation_file_path = os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.dev.tsv\")\n",
        "#test_path\n",
        "test_file_path = os.path.join(os.getcwd(),\"dakshina_dataset_v1.0\",\"hi\",\"lexicons\",\"hi.translit.sampled.test.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TqZUg_I-81Sh"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Preprocessing the words\n",
        "'''\n",
        "def word_process(w):\n",
        "  #adding the tab and next line characters in the words\n",
        "  w = '\\t' + w + '\\n'\n",
        "  return w\n",
        "\n",
        "'''\n",
        "Function - Returns pairs of target word,input word.\n",
        "'''\n",
        "def create_dataset(path):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  #creating the word pairs\n",
        "  word_pairs = [[word_process(w)  for w in line.split('\\t')[:-1]]\n",
        "                for line in lines[:-1]]\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "'''\n",
        "Function - tokenize the language\n",
        "'''\n",
        "def tokenize(lang):\n",
        "  #using keras text tokenizer\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  #generating the sequence\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  #tensor used for pading the sequences\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "  #retun the tensor and the language tokenizer\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "'''\n",
        "Function - load_dataset\n",
        "'''\n",
        "def load_dataset(path):\n",
        "  #creating the target word and input word pairs\n",
        "  output_lang, inp_lang = create_dataset(path)\n",
        "\n",
        "  #generating the tokenized tensor for the input words\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  #generating the tokenized tensor for the target words\n",
        "  output_tensor, output_lang_tokenizer = tokenize(output_lang)\n",
        "\n",
        "  return input_tensor, output_tensor, inp_lang_tokenizer, output_lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mTTPBoWe8j-x"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Reading the training dataset entirely\n",
        "'''\n",
        "# Use the entire training dataset file\n",
        "input_tensor_train, target_tensor_train, inp_lang, targ_lang = load_dataset(train_file_path)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor_train.shape[1], input_tensor_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9aU5v5A68Raa"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Class - GRU Encoder\n",
        "'''\n",
        "class GRU_Encoder(tf.keras.Model):\n",
        "  #Initialization\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout=0):\n",
        "    super(GRU_Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch_size\n",
        "    self.enc_units = enc_units  #encoder_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #embeding dimensions and layer initialization\n",
        "    #keras GRU layer\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = dropout)\n",
        "  #calling the GRU encoder\n",
        "  def call(self, x, hidden):\n",
        "    #calling the embeding initializations\n",
        "    x = self.embedding(x)\n",
        "    #return the encoder output and the state\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  #initialiaztion of the hidden states\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "'''\n",
        "class - LSTM encoder\n",
        "'''\n",
        "class LSTM_Encoder(tf.keras.Model):\n",
        "  #initialization function\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz,dropout=0):\n",
        "    super(LSTM_Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch_size\n",
        "    self.enc_units = enc_units  #encoder_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #embedding_dimensions\n",
        "    #keras LSTM layer\n",
        "    self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                         return_sequences=True,\n",
        "                         return_state=True,\n",
        "                         recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "\n",
        "  #call function\n",
        "  def call(self, x, hidden,cell_state):\n",
        "    #embedding layer calling\n",
        "    x = self.embedding(x)\n",
        "    #output and the last cell calling\n",
        "    output, last_hidden,last_cell_state = self.lstm(x, initial_state=[hidden,cell_state])\n",
        "    #return the output, last hidden and the last cell state\n",
        "    return output, last_hidden,last_cell_state\n",
        "\n",
        "  #initialization of the hidden state\n",
        "  def initialize_hidden_state(self):\n",
        "      return tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "'''\n",
        "class - RNN encoder\n",
        "'''\n",
        "class RNN_Encoder(tf.keras.Model):\n",
        "  #intialization function\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz,dropout=0):\n",
        "    super(RNN_Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch size\n",
        "    self.enc_units = enc_units  #encoder units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)   #embedding dimensions\n",
        "    #keras RNN layer\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(self.enc_units,\n",
        "                         return_sequences=True,\n",
        "                         return_state=True,\n",
        "                         recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "\n",
        "  #call function\n",
        "  def call(self, x, hidden):\n",
        "    #embedding layer calling\n",
        "    x = self.embedding(x)\n",
        "    #returning the output and the final state\n",
        "    output, final_state = self.rnn(x,initial_state=hidden)\n",
        "    return output, final_state\n",
        "\n",
        "  #initialization of the hidden states\n",
        "  def initialize_hidden_state(self):\n",
        "      return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C3C9skWL8-If"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Attention class (Bhadanau Attention) refernce for the attention  - https://arxiv.org/abs/1409.0473\n",
        "'''\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  #initialization\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)  #W_1\n",
        "    self.W2 = tf.keras.layers.Dense(units)  #W_2\n",
        "    self.V = tf.keras.layers.Dense(1)       #V\n",
        "\n",
        "  '''\n",
        "  call function genrating the context vector and the attention weights\n",
        "  '''\n",
        "  def call(self, query, values):\n",
        "    '''\n",
        "    shape of query hidden state == (batch_size, hidden size)\n",
        "    shape of query_with_time_axis == (batch_size, 1, hidden size)\n",
        "    shape of values  == (batch_size, max_len, hidden size)\n",
        "    '''\n",
        "    #To broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # shape of score == (batch_size, max_length, 1)\n",
        "    # shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # shape of attention_weights == (batch_size, max_length, 1)\n",
        "    #generating the attention weights\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    #generating the context vector\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    #returning the context vector and the attention weights\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Snk7YcBD9AQy"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class GRU Decoder\n",
        "'''\n",
        "class GRU_Decoder(tf.keras.Model):\n",
        "  #initialization\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,dropout=0):\n",
        "    super(GRU_Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch size\n",
        "    self.dec_units = dec_units  #decoder units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #embeding layer initialization\n",
        "    #keras GRU layer\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',dropout=dropout)\n",
        "    #dense or fully connected layer\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    #using the attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  #call function to generate the output, state and the attention weights\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    # output shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    output = self.embedding(x)\n",
        "    # output shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    output = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(output)\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output shape == (batch_size, vocab)\n",
        "    output = self.fc(output)\n",
        "    #return the output, state and the attention weights.\n",
        "    return output, state, attention_weights\n",
        "\n",
        "'''\n",
        "class LSTM decoder\n",
        "'''\n",
        "class LSTM_Decoder(tf.keras.Model):\n",
        "  #initialization\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,dropout=0):\n",
        "    super(LSTM_Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch size\n",
        "    self.dec_units = dec_units  #decoder units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #embedding\n",
        "    #keras LSTM layer\n",
        "    self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',dropout=dropout)\n",
        "    #dense/ Fully connected layer\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    #applying the attention layer\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  #call function generating output, hiddden and cell state and the attention weights\n",
        "  def call(self, x, hidden, enc_output,cell_state):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    output = self.embedding(x)\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    output = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, last_hidden_state,last_cell_state = self.lstm(output,initial_state=[hidden,cell_state])\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output shape == (batch_size, vocab)\n",
        "    output = self.fc(output)\n",
        "    #returning output, hiddden and cell state and the attention weights\n",
        "    return output, [last_hidden_state,last_cell_state], attention_weights\n",
        "\n",
        "'''\n",
        "Class RNN decoder\n",
        "'''\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "  #initialization\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,dropout=0):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz    #batch size\n",
        "    self.dec_units = dec_units  #decoder unnits\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #embedding layer\n",
        "    #keras RNN layer\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(self.dec_units,\n",
        "                         return_sequences=True,\n",
        "                         return_state=True,\n",
        "                         recurrent_initializer='glorot_uniform',\n",
        "                         dropout = dropout)\n",
        "    #dense/ fully connected layer\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    #applying attention layer\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "\n",
        "  #call function generating the output state and the attention weights\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    output = self.embedding(x)\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    output = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, final_state = self.rnn(output)\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output shape == (batch_size, vocab)\n",
        "    output = self.fc(output)\n",
        "    #return the output state and the attention weights\n",
        "    return output, final_state, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hPS--UX69DAJ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Fucntion - Calculating the loss function\n",
        "Reference: https://stackoverflow.com/questions/62916592/loss-function-for-sequences-in-tensorflow-2-0\n",
        "'''\n",
        "def calculate_loss(real, pred):\n",
        "  mask_position = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_value = loss_object(real, pred)\n",
        "\n",
        "  mask_position = tf.cast(mask_position, dtype=loss_value.dtype)\n",
        "  loss_value *= mask_position\n",
        "\n",
        "  #returns the mean of the loss value\n",
        "  return tf.reduce_mean(loss_value)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout,train_input_batch):\n",
        "    #Encoder\n",
        "    if rnn_type=='GRU': #GRU\n",
        "       encoder = GRU_Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "       sample_hidden = encoder.initialize_hidden_state()\n",
        "       sample_output, sample_hidden = encoder(train_input_batch, sample_hidden)\n",
        "    elif rnn_type=='LSTM': #LSTM\n",
        "      encoder = LSTM_Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_hidden,sample_cell_state = encoder.initialize_hidden_state()\n",
        "      sample_output, sample_hidden,sample_cell_state = encoder(train_input_batch, sample_hidden,sample_cell_state)\n",
        "    elif rnn_type=='RNN': #RNN\n",
        "      encoder = RNN_Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_hidden = encoder.initialize_hidden_state()\n",
        "      sample_output, sample_hidden = encoder(train_input_batch, sample_hidden)\n",
        "    #printing the shapes\n",
        "    print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
        "    print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)\n",
        "\n",
        "    #Decoder\n",
        "    if rnn_type=='GRU': #GRU\n",
        "      decoder = GRU_Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "\n",
        "    elif rnn_type=='LSTM': #LSTM\n",
        "      decoder = LSTM_Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output, sample_cell_state)\n",
        "\n",
        "    elif rnn_type=='RNN': #RNN\n",
        "      decoder = RNN_Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout)\n",
        "      sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "\n",
        "    #print the decoder shape\n",
        "    print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)\n",
        "\n",
        "    return encoder,decoder"
      ],
      "metadata": {
        "id": "xGUVdtvkNyTS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epochs(EPOCHS,encoder,decoder,dataset,steps_per_epoch):\n",
        "    train_loss=[0]*EPOCHS\n",
        "    for epoch in range(EPOCHS):\n",
        "      start = time.time()\n",
        "      if rnn_type!='LSTM': #GRU or RNN\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "      elif rnn_type=='LSTM': #LSTM\n",
        "        enc_hidden,enc_cell_state = encoder.initialize_hidden_state()\n",
        "      total_loss = 0\n",
        "      for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        #Training for every batch\n",
        "        if rnn_type!='LSTM':\n",
        "          batch_loss = train_batch(inp, targ, enc_hidden, encoder,decoder,rnn_type)\n",
        "        elif rnn_type=='LSTM':\n",
        "          batch_loss = train_batch(inp, targ, [enc_hidden,enc_cell_state], encoder,decoder,rnn_type)\n",
        "        total_loss += batch_loss\n",
        "      if batch % 100 == 0:\n",
        "        #printing the batch loss\n",
        "        print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "\n",
        "      # saving (checkpoint) the model every 2 epochs\n",
        "      if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "      #print the trtaining loss for the epoch\n",
        "      print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "      print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "      # Storing the average loss per epoch\n",
        "      train_loss[epoch] = total_loss.numpy()/steps_per_epoch\n",
        "\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "t0hlWYxWaq9Q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FEuZduf39E2J"
      },
      "outputs": [],
      "source": [
        "def train(use_wandb=True):\n",
        "    global BATCH_SIZE\n",
        "    global units\n",
        "    global vocab_inp_size\n",
        "    global vocab_tar_size\n",
        "    global embedding_dim\n",
        "    global encoder\n",
        "    global decoder\n",
        "    global optimizer\n",
        "    global loss_object\n",
        "    global checkpoint_dir\n",
        "    global checkpoint_prefix\n",
        "    global checkpoint\n",
        "    global run_name\n",
        "    global rnn_type\n",
        "\n",
        "\n",
        "    '''\n",
        "    Wandb configuration\n",
        "    '''\n",
        "    # initialising the wandb run\n",
        "    run = wandb.init()\n",
        "    # Tpye of RNN to choose. Acceptable Values are 'RNN'. 'LSTM' and 'GRU'\n",
        "    rnn_type = run.config.rnn_type\n",
        "    # Batch size for training.\n",
        "    BATCH_SIZE = run.config.bs\n",
        "    # Dimensions of the abstract representation of the input word and target word.\n",
        "    embedding_dim = run.config.embed\n",
        "    # Latent dimensions of the encoder and decoder.\n",
        "    units = run.config.latent\n",
        "    # Number of epochs to train for.\n",
        "    EPOCHS = run.config.epochs\n",
        "    #\tFloat between 0 and 1. Denotes the fraction of the units to drop.\n",
        "    dropout = run.config.dropout\n",
        "\n",
        "\n",
        "    print(\"rnn_Type: \",rnn_type)\n",
        "    #buffer size\n",
        "    BUFFER_SIZE = len(input_tensor_train)\n",
        "    #steps per epoch\n",
        "    steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "    #vocab input size\n",
        "    vocab_inp_size = len(inp_lang.word_index)+1\n",
        "    #vocab target size\n",
        "    vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "    #wandb run name\n",
        "    run_name = '_epochs_'+str(EPOCHS)+'_rnn_type_'+str(rnn_type)+'_bs_'+str(BATCH_SIZE)+'_embed_'+str(embedding_dim)+'_latent_'+str(units)+'_dropout_'+str(dropout)\n",
        "    if use_wandb==True:\n",
        "      wandb.run.name = run_name\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "    # We create batches of size BATCH_SIZE and ignore the last batch because the last batch may not be equal to BATCH_SIZE\n",
        "    #creating the batches\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "    train_input_batch, train_target_batch = next(iter(dataset))\n",
        "\n",
        "    encoder, decoder = build_model(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout,train_input_batch)\n",
        "\n",
        "    #Apply the adam optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "    #saving the checkpoints\n",
        "    checkpoint_dir = os.path.join(os.getcwd(),'training_checkpoints')\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "\n",
        "    ####################################################################### Hyper parameter Tuning (Training) ###############################################################\n",
        "\n",
        "    train_loss=[0]*EPOCHS\n",
        "    for epoch in range(EPOCHS):\n",
        "      start = time.time()\n",
        "      if rnn_type!='LSTM': #GRU or RNN\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "      elif rnn_type=='LSTM': #LSTM\n",
        "        enc_hidden,enc_cell_state = encoder.initialize_hidden_state()\n",
        "      total_loss = 0\n",
        "      for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        #Training for every batch\n",
        "        if rnn_type!='LSTM':\n",
        "          batch_loss = train_batch(inp, targ, enc_hidden, encoder,decoder,rnn_type)\n",
        "        elif rnn_type=='LSTM':\n",
        "          batch_loss = train_batch(inp, targ, [enc_hidden,enc_cell_state], encoder,decoder,rnn_type)\n",
        "        total_loss += batch_loss\n",
        "      if batch % 100 == 0:\n",
        "        #printing the batch loss\n",
        "        print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "\n",
        "      # saving (checkpoint) the model every 2 epochs\n",
        "      if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "      #print the trtaining loss for the epoch\n",
        "      print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "      print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "      # Storing the average loss per epoch\n",
        "      train_loss[epoch] = total_loss.numpy()/steps_per_epoch\n",
        "      #logging the training loss in wandb\n",
        "      if use_wandb == True:\n",
        "        wandb.log({\"train_loss\": total_loss.numpy()/steps_per_epoch})\n",
        "\n",
        "\n",
        "    #test_accuracy = validate(test_file_path,run_name)\n",
        "    val_acc=validate(vaildation_file_path,rnn_type)\n",
        "    print(\"Train loss: \",train_loss)\n",
        "    print(\"Validation Accuracy: \",val_acc)\n",
        "    #print(\"Test Accuracy: \",test_accuracy)\n",
        "    if use_wandb ==True:\n",
        "      wandb.log({'val_accuracy': val_acc})\n",
        "\n",
        "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MQW3p5ETAFCu"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function for training manually the best configuration of the model\n",
        "'''\n",
        "def manual_train():\n",
        "    global BATCH_SIZE\n",
        "    global units\n",
        "    global vocab_inp_size\n",
        "    global vocab_tar_size\n",
        "    global embedding_dim\n",
        "    global encoder\n",
        "    global decoder\n",
        "    global optimizer\n",
        "    global loss_object\n",
        "    global checkpoint_dir\n",
        "    global checkpoint_prefix\n",
        "    global checkpoint\n",
        "    global run_name\n",
        "    global rnn_type\n",
        "\n",
        "    '''\n",
        "    Best configuration of the model\n",
        "    '''\n",
        "    rnn_type = 'LSTM'\n",
        "    BATCH_SIZE = 64\n",
        "    embedding_dim = 512\n",
        "    units = 1024\n",
        "    EPOCHS = 20\n",
        "    dropout = 0.2\n",
        "\n",
        "    print(\"rnn_Type: \",rnn_type)\n",
        "    #generating the buffer size\n",
        "    BUFFER_SIZE = len(input_tensor_train)\n",
        "    #calculating the number of steps per epoch\n",
        "    steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "    #vocab input size\n",
        "    vocab_inp_size = len(inp_lang.word_index)+1\n",
        "    #vocab target size\n",
        "    vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "    run_name = '_epochs_'+str(EPOCHS)+'_rnn_type_'+str(rnn_type)+'_bs_'+str(BATCH_SIZE)+'_embed_'+str(embedding_dim)+'_latent_'+str(units)+'_dropout_'+str(dropout)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "    #We are creating batches of size BATCH_SIZE and ignore the last batch because the last batch may not be equal to BATCH_SIZE\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "    train_input_batch, train_target_batch = next(iter(dataset))\n",
        "\n",
        "    encoder, decoder = build_model(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout,train_input_batch)\n",
        "\n",
        "    #apply the adam optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "    #creating and saving the checkpoints\n",
        "    checkpoint_dir = os.path.join(os.getcwd(),'training_checkpoints')\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "\n",
        "    train_loss=[0]*EPOCHS\n",
        "\n",
        "    ############################################################################## Training ##########################################################################\n",
        "\n",
        "    train_loss = train_epochs(EPOCHS,encoder,decoder,dataset,steps_per_epoch)\n",
        "\n",
        "\n",
        "    #calculating the test accuracy using the validate function\n",
        "    test_accuracy = validate(test_file_path,run_name)\n",
        "    #calcualting the validation accuracy using validate function\n",
        "    val_acc=validate(vaildation_file_path,rnn_type)\n",
        "    print(\"Train loss: \",train_loss)\n",
        "    print(\"Validation Accuracy: \",val_acc)\n",
        "    print(\"Test Accuracy: \",test_accuracy)\n",
        "\n",
        " \t  # restoring the latest checkpoint in checkpoint_dir and starting the test\n",
        "  \t# checkpoints are only useful when source code that will use the saved parameter values is available.\n",
        "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "    generate_inputs(rnn_type,10)\n",
        "    #generating the connectivity of the best model.\n",
        "    connectivity(['maryaadaa','prayogshala','angarakshak'],rnn_type, os.path.join(os.getcwd(),\"predictions_attention\",str(run_name)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kqfHJ7e59Kpr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function - train_batch\n",
        "calculates the loss of the batch and returns the batch loss after training every batch in each epoch\n",
        "'''\n",
        "@tf.function\n",
        "def train_batch(inp, targ, enc_hidden, enocder, decoder,rnn_type):\n",
        "  #Final loss value\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "        #checking if it is GRU or RNN\n",
        "        if rnn_type!='LSTM':\n",
        "            enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "            dec_hidden = enc_hidden\n",
        "        #checking if it is LSTM\n",
        "        elif rnn_type=='LSTM':\n",
        "            enc_output, enc_hidden,enc_cell_state = encoder(inp, enc_hidden[0],enc_hidden[1])\n",
        "            dec_hidden = enc_hidden\n",
        "            dec_cell_state=enc_cell_state\n",
        "\n",
        "        #geting the decoder input\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['\\t']] * BATCH_SIZE, 1)\n",
        "\n",
        "        #Teacher forcing\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            if rnn_type!='LSTM':\n",
        "                # passing enc_output to the decoder if it is RNN or GRU\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            elif rnn_type=='LSTM':\n",
        "                if t==1:\n",
        "                  # passing enc_output to the decoder if it is a LSTM\n",
        "                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output,dec_cell_state)\n",
        "                elif t>1:\n",
        "                  # passing enc_output to the decoder if it is a LSTM\n",
        "                  predictions, dec_hidden, _ = decoder(dec_input, dec_hidden[0], enc_output,dec_cell_state)\n",
        "            #calculating the loss using calculate loss function\n",
        "            loss += calculate_loss(targ[:, t], predictions)\n",
        "            #using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  #calculating the batch loss\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  #calculate the variables\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  #calculate the gradients\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  #applying the gradients to the optimizer\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3KqJkihJ9PT4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function - inference_model\n",
        "generating the predicted word, input word, attention weights and the attention plot\n",
        "'''\n",
        "def inference_model(input_word,rnn_type):\n",
        "  #creating an empty attention plot\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  #preprocessing the input word\n",
        "  input_word = word_process(input_word)\n",
        "\n",
        "  #converting the word to tensor after pading\n",
        "  inputs = [inp_lang.word_index[i] for i in input_word]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  #predicted word initialization\n",
        "  predicted_word = ''\n",
        "\n",
        "  #if cell type is GRU or RNN\n",
        "  if rnn_type!='LSTM':\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "  #if cell type is LSTM\n",
        "  elif rnn_type=='LSTM':\n",
        "    hidden=tf.zeros((1, units))\n",
        "    cell_state= tf.zeros((1, units))\n",
        "    enc_out, enc_hidden,enc_cell_state = encoder(inputs, hidden,cell_state)\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "  #generating the decode inputs\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['\\t']], 0)\n",
        "\n",
        "  #storing the attention weights\n",
        "  att_w=[]\n",
        "\n",
        "  #calculating the predictions\n",
        "  for t in range(max_length_targ):\n",
        "    #if cell is GRU or RNN\n",
        "    if rnn_type!='LSTM':\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input,dec_hidden,enc_out)\n",
        "    #if cell is LSTM\n",
        "    elif rnn_type=='LSTM':\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out, enc_cell_state)\n",
        "      dec_hidden=dec_hidden[0]\n",
        "\n",
        "    # storing the attention weights for plotting latter\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    att_w.append(attention_weights.numpy()[0:len(input_word)])\n",
        "\n",
        "\n",
        "    #predicted id\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    #predicted word\n",
        "    predicted_word += targ_lang.index_word[predicted_id]\n",
        "\n",
        "    #in case of last character\n",
        "    if targ_lang.index_word[predicted_id] == '\\n':\n",
        "      return predicted_word, input_word, attention_plot,att_w\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "  #finally return the predicted word, input word, attention plot and the attention weight\n",
        "  return predicted_word, input_word, attention_plot,att_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DVHrrxwt9Ro5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function - Validate\n",
        "returning the validation or the testing accuracy on the validation data\n",
        "'''\n",
        "def validate(path_to_file,folder_name):\n",
        "  #while testing to generate the predictions files (output files)\n",
        "  save = False\n",
        "  if path_to_file.find(\"test\")!=-1:\n",
        "    if os.path.exists(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name))):\n",
        "      shutil.rmtree(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name)))\n",
        "\n",
        "    if not os.path.exists(os.path.join(os.getcwd(),\"predictions_attention\")):\n",
        "        os.mkdir(os.path.join(os.getcwd(),\"predictions_attention\"))\n",
        "    os.mkdir(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name)))\n",
        "    success_file = open(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name),\"success.txt\"),\"w\",encoding='utf-8', errors='ignore')\n",
        "    failure_file = open(os.path.join(os.getcwd(),\"predictions_attention\",str(folder_name),\"failure.txt\"),\"w\",encoding='utf-8', errors='ignore')\n",
        "    save=True\n",
        "\n",
        "  #the count of the correct predictions\n",
        "  success_count=0\n",
        "  # Get the target words and input words for the validation\n",
        "  target_words, input_words = create_dataset(path_to_file)\n",
        "  for i in range(len(input_words)):\n",
        "    #generate the predicted words for the corresponding input words\n",
        "    predicted_word, input_word, attention_plot,att_w = inference_model(input_words[i],rnn_type)\n",
        "    record= input_word.strip()+' '+target_words[i].strip()+' '+predicted_word[:-1].strip()+\"\\n\"\n",
        "    # The last character of target_words[i] and predicted word is '\\n', first character of target_words[i] is '\\t'\n",
        "    if target_words[i][1:]==predicted_word:\n",
        "      #increasing the accuracy count\n",
        "      success_count = success_count + 1\n",
        "      if save == True:\n",
        "        success_file.write(record)\n",
        "    elif save==True:\n",
        "      failure_file.write(record)\n",
        "\n",
        "  #saving the files\n",
        "  if save==True:\n",
        "    success_file.close()\n",
        "    failure_file.close()\n",
        "\n",
        "  #return the acuracy\n",
        "  return success_count/len(input_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MoLXeHzI9VBr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function  - plot - attention\n",
        "Function ploting the attention plots.\n",
        "'''\n",
        "def plot_attention(attention, input_word, predicted_word, file_name):\n",
        "  #loading the hindi font for displaying\n",
        "  hindi_font = FontProperties(fname = os.path.join(os.getcwd(),\"Nirmala.ttf\"))\n",
        "\n",
        "  #figure matplotlib\n",
        "  fig = plt.figure(figsize=(3, 3))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + list(input_word), fontdict=fontdict, rotation=0)\n",
        "  ax.set_yticklabels([''] + list(predicted_word), fontdict=fontdict,fontproperties=hindi_font)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  #save the plot figure.\n",
        "  plt.savefig(file_name)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oJHKHIcC9ijI"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Geting the connectivity html file.\n",
        "'''\n",
        "# get html element\n",
        "def cstr(s, color='black'):\n",
        "\tif s == ' ':\n",
        "\t\treturn \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
        "\telse:\n",
        "\t\treturn \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
        "\n",
        "# print html\n",
        "def print_color(t):\n",
        "\tdisplay(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
        "\n",
        "# get appropriate color for value\n",
        "# Darker shades of green denotes higher importance.\n",
        "def get_clr(value):\n",
        "\tcolors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8',\n",
        "\t\t'#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
        "\t\t'#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
        "\t\t'#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
        "\tvalue = int((value * 100) / 5)\n",
        "\treturn colors[value]\n",
        "\n",
        "\n",
        "'''\n",
        "Function - Visualize the connectivity plots in the HTMl file\n",
        "'''\n",
        "def visualize(input_word, output_word, att_w):\n",
        "  for i in range(len(output_word)):\n",
        "    print(\"\\nOutput character:\", output_word[i], \"\\n\")\n",
        "    text_colours = []\n",
        "    for j in range(len(att_w[i])):\n",
        "      text = (input_word[j], get_clr(att_w[i][j]))\n",
        "      text_colours.append(text)\n",
        "    print_color(text_colours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QvyqHnYQ9ka_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Code for connectivity visualisation.\n",
        "'''\n",
        "# get appropriate color for value\n",
        "# Darker shades of green denotes higher importance.\n",
        "def get_shade_color(value):\n",
        "\tcolors = ['#00fa00', '#00f500',  '#00eb00', '#00e000',  '#00db00',\n",
        "           '#00d100',  '#00c700',  '#00c200', '#00b800',  '#00ad00',\n",
        "           '#00a800',  '#009e00',  '#009400', '#008f00',  '#008500',\n",
        "           '#007500',  '#007000',  '#006600', '#006100',  '#005c00',\n",
        "           '#005200',  '#004d00',  '#004700', '#003d00',  '#003800',\n",
        "           '#003300',  '#002900',  '#002400',  '#001f00',  '#001400']\n",
        "\tvalue = int((value * 100) / 5)\n",
        "\treturn colors[value]\n",
        "\n",
        "#creating the HTMl file\n",
        "def create_file(text_colors,input_word,output_word,file_path=os.getcwd()):\n",
        "  text = '''\n",
        "  <!DOCTYPE html>\n",
        "  <html>\n",
        "  <head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
        "    <script>\n",
        "            $(document).ready(function(){\n",
        "            var col =['''\n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word)):\n",
        "              text=text+'''['''\n",
        "              for j in range(len(text_colors[k][i])-1):\n",
        "                text=text+'''\\\"'''+text_colors[k][i][j]+'''\\\"'''+''','''\n",
        "              text=text+'''\\\"'''+text_colors[k][i][len(text_colors[k][i])-1]+'''\\\"'''+'''],'''\n",
        "  text=text[0:-1]\n",
        "  text=text+'''];\\n'''\n",
        "\n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word[k])):\n",
        "            text=text+'''$(\\\".h'''+str(k)+str(i)+'''\\\").mouseover(function(){\\n'''\n",
        "            for j in range(len(input_word[k])):\n",
        "                       text=text+'''$(\\\".t'''+str(k)+str(j)+'''\\\").css(\\\"background-color\\\", col['''+str(i)+''']'''+'''['''+str(j)+''']);\\n'''\n",
        "            text=text+'''});\\n'''\n",
        "            text=text+'''$(\\\".h'''+str(k)+str(i)+'''\\\").mouseout(function(){\\n'''\n",
        "            for l in range(3):\n",
        "              for j in range(len(input_word[l])):\n",
        "                text=text+'''$(\\\".t'''+str(l)+str(j)+'''\\\").css(\\\"background-color\\\", \\\"#ffff99\\\");\\n'''\n",
        "            text=text+'''});\\n'''\n",
        "  text=text+'''});\\n\n",
        "</script>\n",
        "  </head>\n",
        "      <body>\n",
        "          <h1>Connectivity:</h1>\n",
        "          <p> The connection strength between the target for the selected character and the input characters is highlighted in green (reset). Hover over the text to change the selected character.</p>\n",
        "          <div style=\"background-color:#ffff99;color:black;padding:2%; margin:4%;\">\n",
        "          <p>\n",
        "          <div> Output: </div>\n",
        "          <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>\n",
        "          '''\n",
        "  for k in range(3):\n",
        "      for i in range(len(output_word[k])):\n",
        "            text=text+'''\\n'''+'''\\t'''+'''<div class=\"h'''+str(k)+str(i)+'''\\\">'''+output_word[k][i]+'''</div>'''\n",
        "      text=text+'''</div>'''+'\\n'+'\\t'+'''<div>  </p>'''+'\\n'+'\\t'+'''<p>\n",
        "      <div> Input: </div>\n",
        "      <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>'''\n",
        "      for j in range(len(input_word[k])):\n",
        "        text=text+'''\\n'''+'''\\t'''+'''<div class=\"t'''+str(k)+str(j)+'''\\\">'''+input_word[k][j]+'''</div>'''\n",
        "      if k<2:\n",
        "          text = text+'''</div></p></div><p></p></div>\n",
        "          <div style=\"background-color:#ffff99;color:black;padding:2%; margin:4%;\">\n",
        "          <div> Output: </div>\n",
        "          <div style='display:flex; border: 2px solid #d0cccc; padding: 8px; margin: 8px;'>'''\n",
        "  text=text+'''\n",
        "        </div>\n",
        "        </p>\n",
        "        </div>\n",
        "        </body>\n",
        "  </html>\n",
        "  '''\n",
        "  fname = os.path.join(file_path,\"connectivity.html\")\n",
        "  file = open(fname,\"w\")\n",
        "  file.write(text)\n",
        "  file.close()\n",
        "\n",
        "#main file to generate the connectivity of HTML file\n",
        "def connectivity(input_words,rnn_type,file_path):\n",
        "  #color list\n",
        "  color_list=[]\n",
        "  #input word list\n",
        "  input_word_list=[]\n",
        "  #output word list\n",
        "  output_word_list=[]\n",
        "\n",
        "  for k in range(3):\n",
        "    #do inferencing in the model\n",
        "    output_word, input_word, _ ,att_w = inference_model(input_words[k],rnn_type)\n",
        "    text_colours=[]\n",
        "    for i in range(len(output_word)):\n",
        "      colour=[]\n",
        "      for j in range(len(att_w[i])):\n",
        "        value=get_shade_color(att_w[i][j])\n",
        "        colour.append(value)\n",
        "      text_colours.append(colour)\n",
        "    #creating the color list\n",
        "    color_list.append(text_colours)\n",
        "    #input word list\n",
        "    input_word_list.append(input_word)\n",
        "    #output word list\n",
        "    output_word_list.append(output_word)\n",
        "  #create file for generating the HTML file\n",
        "  create_file(color_list,input_word_list,output_word_list,file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "scx1WsVh9mEV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function - Transliteration\n",
        "generating the attention heatmap\n",
        "'''\n",
        "def transliterate(input_word,rnn_type,file_name=os.path.join(os.getcwd(),\"attention_heatmap.png\"),visual_flag=True):\n",
        "  #do inferencing ot get the predicted word and other attention plots, weights and input word\n",
        "  predicted_word, input_word, attention_plot,att_w = inference_model(input_word,rnn_type)\n",
        "\n",
        "  #geting the predicted transliterations\n",
        "  print(\"\\n\",'Input:', input_word)\n",
        "  print('Predicted transliteration:', predicted_word)\n",
        "\n",
        "  attention_plot = attention_plot[:len(predicted_word),\n",
        "                                  :len(input_word)]\n",
        "  plot_attention(attention_plot, input_word, predicted_word, file_name)\n",
        "\n",
        "  if visual_flag == True:\n",
        "    #visualize the attention plots\n",
        "    visualize(input_word, predicted_word, att_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nxsCx9iU9n6m"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Fucntion - generate_inputs\n",
        "generate the predictions for the attention based model\n",
        "'''\n",
        "def generate_inputs(rnn_type,n_test_samples=10):\n",
        "  target_words, input_words = create_dataset(test_file_path)\n",
        "\n",
        "  for i in range (n_test_samples):\n",
        "    index = random.randint(0,len(input_words))\n",
        "    input_word=input_words[index]\n",
        "    file_name=os.path.join(os.getcwd(),\"predictions_attention\",str(run_name),input_word+\".png\")\n",
        "\n",
        "    if i == 0:\n",
        "      transliterate(input_word[1:-1],rnn_type, file_name,True)\n",
        "    elif i > 0:\n",
        "      transliterate(input_word[1:-1],rnn_type, file_name,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRKjcUDR-r1F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "outputId": "2a7e6d1a-1138-4fc3-f85c-c2635bd95e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m034\u001b[0m (\u001b[33mcs24m034-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 0f828mox\n",
            "Sweep URL: https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention/sweeps/0f828mox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eeqau225 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbs: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: LSTM\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250519_012241-eeqau225</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention/runs/eeqau225' target=\"_blank\">expert-sweep-1</a></strong> to <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention/sweeps/0f828mox' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention/sweeps/0f828mox</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention/sweeps/0f828mox' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention/sweeps/0f828mox</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention/runs/eeqau225' target=\"_blank\">https://wandb.ai/cs24m034-indian-institute-of-technology-madras/DA6401_Assignment3_attention/runs/eeqau225</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rnn_Type:  LSTM\n",
            "Encoder output shape: (batch size, sequence length, units) (64, 22, 512)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 512)\n",
            "Decoder output shape: (batch_size, vocab size) (64, 65)\n",
            "Epoch 1 Loss 0.4025\n",
            "Time taken for 1 epoch 78.18 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Sweep configuration for hyper parameter tuning\n",
        "'''\n",
        "!pip install wandb --upgrade\n",
        "import wandb\n",
        "# !wandb login\n",
        "wandb.login(key=\"7f46816d45e3df192c3053bab59032e9d710fef4\")\n",
        "sweep_config = {\n",
        "    \"name\": \"Bayesian Sweep without attention\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "\n",
        "        \"rnn_type\": {\"values\": [\"RNN\",\"GRU\",\"LSTM\"]},\n",
        "\n",
        "        \"embed\": {\"values\": [256,512]},\n",
        "\n",
        "        \"latent\": {\"values\": [512,1024]},\n",
        "\n",
        "        \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n",
        "\n",
        "        \"epochs\": {\"values\": [20]},\n",
        "\n",
        "        \"bs\": {\"values\": [64]},\n",
        "\n",
        "\n",
        "    },\n",
        "  }\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"DA6401_Assignment3_attention\", entity=\"cs24m034-indian-institute-of-technology-madras\")\n",
        "\n",
        "wandb.agent(sweep_id, train, count = 30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "manual training for the best parameter model\n",
        "'''\n",
        "manual_train()"
      ],
      "metadata": {
        "id": "Q3qXCUCWPPRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZfLraCi9tER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "67f172f9-3676-40b5-fa34-c8d872d25849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/predictions_attention/ (stored 0%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/ (stored 0%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ibolane^J.png (deflated 11%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ipharmacy^J.png (deflated 8%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/connectivity.html (deflated 96%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ibhagane^J.png (deflated 12%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ijoddta^J.png (deflated 11%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Inikalegi^J.png (deflated 8%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Imahanatam^J.png (deflated 10%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Iagr^J.png (deflated 13%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ikatthaii^J.png (deflated 9%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/success.txt (deflated 79%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/failure.txt (deflated 76%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Imamuli^J.png (deflated 12%)\n",
            "  adding: content/predictions_attention/_epochs_20_rnn_type_LSTM_bs_64_embed_512_latent_1024_dropout_0.2/^Ivishahin^J.png (deflated 9%)\n",
            "  adding: content/training_checkpoints/ (stored 0%)\n",
            "  adding: content/training_checkpoints/ckpt-7.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-5.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-10.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-9.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-1.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-4.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-9.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-8.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/checkpoint (deflated 38%)\n",
            "  adding: content/training_checkpoints/ckpt-8.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-10.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-7.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-4.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-3.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-2.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-6.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-3.index (deflated 69%)\n",
            "  adding: content/training_checkpoints/ckpt-2.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-6.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-1.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/training_checkpoints/ckpt-5.index (deflated 69%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_61960e6f-795e-4675-81cd-591bab762e1c\", \"predictions_attention.zip\", 84504)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_72c2bca5-bde4-43d0-b21f-28fd7c401137\", \"training_checkpoints.zip\", 2119938640)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Download a copy of the predictions_attention and training_checkpoints folder.\n",
        "!zip -r /content/predictions_attention.zip /content/predictions_attention\n",
        "!zip -r /content/training_checkpoints.zip /content/training_checkpoints\n",
        "from google.colab import files\n",
        "files.download(\"/content/predictions_attention.zip\")\n",
        "files.download(\"/content/training_checkpoints.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fdPIgZbocyn4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}